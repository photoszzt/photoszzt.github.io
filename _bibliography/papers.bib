@inproceedings{10.1145/3689031.3717485,
author = {Zhu, Zhiting and Jia, Zhipeng and Ni, Newton and Tang, Dixin and Witchel, Emmett},
title = {Impeller: Stream Processing on Shared Logs},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689031.3717485},
doi = {10.1145/3689031.3717485},
abstract = {Current stream processing systems provide exactly-once semantics using checkpointing or a combination of logging and checkpointing. These approaches can introduce high overhead, significantly increasing the latency for normal stream processing because maintaining exactly-once semantics requires coordination across distributed nodes and streams to capture a globally consistent state. We observe that modern distributed shared logs offer a promising solution for maintaining exactly-once semantics with a small overhead. We propose Impeller, a stream processing system that uses a distributed shared log for data storage and exactly-once processing. To maintain exactly-once semantics, Impeller includes a novel and efficient progress marking protocol based on string tags and selective reads in a shared log. The key idea is to leverage the log's record-tagging feature to atomically mark progress across all streams. The experiments over the NEXMark benchmark show that Impeller achieves 1.3\texttimes{} to 5.4\texttimes{} lower p50 latency, or 1.3\texttimes{} to 5.0\texttimes{} higher saturation throughput than Kafka Streams.},
booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
pages = {637–653},
numpages = {17},
location = {Rotterdam, Netherlands},
series = {EuroSys '25},
pdf = {https://dl.acm.org/doi/pdf/10.1145/3689031.3717485},
code = {https://github.com/ut-osa/impeller-artifact},
}

@inproceedings{10.1145/3698783.3699377,
author = {Zhu, Zhiting and Ni, Newton and Huang, Yibo and Sun, Yan and Jia, Zhipeng and Kim, Nam Sung and Witchel, Emmett},
title = {Lupin: Tolerating Partial Failures in a CXL Pod},
year = {2024},
isbn = {9798400713033},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698783.3699377},
doi = {10.1145/3698783.3699377},
abstract = {A compute express link (CXL) pod is a collection of hosts attached to a CXL memory module. It provides an opportunity to port single-host shared-memory programs to execute on multiple hosts in a CXL pod, where the ported application achieves higher performance than a distributed application that uses network for coordination. The cost of performance scaling on a CXL pod is that applications should tolerate partial failures, where one process or operating system fails or reboots. Lupin is system software that includes kernel modifications and user-level libraries to help applications remain available while they recover from partial failures using the contents of CXL memory.},
booktitle = {Proceedings of the 2nd Workshop on Disruptive Memory Systems},
pages = {41–50},
numpages = {10},
keywords = {CXL, Partial failure tolerance},
location = {Austin, TX, USA},
series = {DIMES '24},
pdf = {https://dl.acm.org/doi/pdf/10.1145/3698783.3699377}
}

@article{10.1145/3606557.3606560,
author = {Fingler, Henrique and Zhu, Zhiting and Yoon, Esther and Jia, Zhipeng and Witchel, Emmett and Rossbach, Christopher J.},
title = {Disaggregated GPU Acceleration for Serverless Applications},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/3606557.3606560},
doi = {10.1145/3606557.3606560},
abstract = {Serverless platforms have been attracting applications from traditional platforms because infrastructure management responsibilities are shifted from users to providers. Many applications well-suited to serverless environments could leverage GPU acceleration to enhance their performance. Unfortunately, current serverless platforms do not expose GPUs to serverless applications.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jun,
pages = {10–20},
numpages = {11}
}

@INPROCEEDINGS{9820659,
  author={Fingler, Henrique and Zhu, Zhiting and Yoon, Esther and Jia, Zhipeng and Witchel, Emmett and Rossbach, Christopher J.},
  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  title={DGSF: Disaggregated GPUs for Serverless Functions},
  year={2022},
  volume={},
  number={},
  pages={739-750},
  keywords={Distributed processing;Cloud computing;Runtime;Graphics processing units;Prototypes;Production;Machine learning;cloud computing;serverless;FaaS;GPU;API remoting},
  doi={10.1109/IPDPS53621.2022.00077},
  abstract={Ease of use and transparent access to elastic resources have attracted many applications away from traditional platforms toward serverless functions. Many of these applications, such as machine learning, could benefit significantly from GPU acceleration. Unfortunately, GPUs remain inaccessible from serverless functions in modern production settings. We present DGSF, a platform that transparently enables serverless functions to use GPUs through general purpose APIs such as CUDA. DGSF solves provisioning and utilization challenges with disaggregation, serving the needs of a potentially large number of functions through virtual GPUs backed by a small pool of physical GPUs on dedicated servers. Disaggregation allows the provider to decouple GPU provisioning from other resources, and enables significant benefits through consolidation. We describe how DGSF solves GPU disaggregation challenges including supporting API transparency, hiding the latency of communication with remote GPUs, and load-balancing access to heavily shared GPUs. Evaluation of our prototype on six workloads shows that DGSF's API remoting optimizations can improve the runtime of a function by up to 50% relative to unoptimized DGSF. Such optimizations, which aggressively remove GPU runtime and object management latency from the critical path, can enable functions running over DGSF to have a lower end-to-end time than when running on a GPU natively. By enabling GPU sharing, DGSF can reduce function queueing latency by up to 53%. We use DGSF to augment AWS Lambda with GPU support, showing similar benefits.}
}

@article{10.1145/3318159,
author = {Hu, Yige and Zhu, Zhiting and Neal, Ian and Kwon, Youngjin and Cheng, Tianyu and Chidambaram, Vijay and Witchel, Emmett},
title = {TxFS: Leveraging File-system Crash Consistency to Provide ACID Transactions},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3318159},
doi = {10.1145/3318159},
abstract = {We introduce TxFS, a transactional file system that builds upon a file system’s atomic-update mechanism such as journaling. Though prior work has explored a number of transactional file systems, TxFS has a unique set of properties: a simple API, portability across different hardware, high performance, low complexity (by building on the file-system journal), and full ACID transactions. We port SQLite, OpenLDAP, and Git to use TxFS and experimentally show that TxFS provides strong crash consistency while providing equal or better performance.},
journal = {ACM Trans. Storage},
month = may,
articleno = {9},
numpages = {20},
keywords = {file systems, crash consistency, Operating systems, ACID transactions}
}

@inproceedings {215985,
author = {Yige Hu and Zhiting Zhu and Ian Neal and Youngjin Kwon and Tianyu Cheng and Vijay Chidambaram and Emmett Witchel},
title = {{TxFS}: Leveraging {File-System} Crash Consistency to Provide {ACID} Transactions},
booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Boston, MA},
pages = {879--891},
url = {https://www.usenix.org/conference/atc18/presentation/hu},
publisher = {USENIX Association},
month = jul,
abstract = {We introduce TxFS, a novel transactional file system that builds upon a file system’s atomic-update mechanism such as journaling. Though prior work has explored a number of transactional file systems, TxFS has a unique set of properties: a simple API, portability across different hardware, high performance, low complexity (by building on the journal), and full ACID transactions. We port SQLite and Git to use TxFS, and experimentally show that TxFS provides strong crash consistency while providing equal or better performance.},
pdf = {https://www.usenix.org/system/files/conference/atc18/atc18-hu.pdf},
}

@article{10.1145/3231594,
author = {Hunt, Tyler and Zhu, Zhiting and Xu, Yuanzhong and Peter, Simon and Witchel, Emmett},
title = {Ryoan: A Distributed Sandbox for Untrusted Computation on Secret Data},
year = {2018},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3231594},
doi = {10.1145/3231594},
abstract = {Users of modern data-processing services such as tax preparation or genomic screening are forced to trust them with data that the users wish to keep secret. Ryoan1 protects secret data while it is processed by services that the data owner does not trust. Accomplishing this goal in a distributed setting is difficult, because the user has no control over the service providers or the computational platform. Confining code to prevent it from leaking secrets is notoriously difficult, but Ryoan benefits from new hardware and a request-oriented data model.Ryoan provides a distributed sandbox, leveraging hardware enclaves (e.g., Intel’s software guard extensions (SGX) [40]) to protect sandbox instances from potentially malicious computing platforms. The protected sandbox instances confine untrusted data-processing modules to prevent leakage of the user’s input data. Ryoan is designed for a request-oriented data model, where confined modules only process input once and do not persist state about the input. We present the design and prototype implementation of Ryoan and evaluate it on a series of challenging problems including email filtering, health analysis, image processing and machine translation.},
journal = {ACM Trans. Comput. Syst.},
month = dec,
articleno = {13},
numpages = {32},
keywords = {Intel SGX, enclaves, private computation, sandboxing, untrusted OS},
code = {https://github.com/ut-osa/ryoan},
}

@inproceedings{10.1145/3038228.3038233,
author = {Zhu, Zhiting and Kim, Sangman and Rozhanski, Yuri and Hu, Yige and Witchel, Emmett and Silberstein, Mark},
title = {Understanding The Security of Discrete GPUs},
year = {2017},
isbn = {9781450349154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3038228.3038233},
doi = {10.1145/3038228.3038233},
abstract = {GPUs have become an integral part of modern systems, but their implications for system security are not yet clear. This paper demonstrates both that discrete GPUs cannot be used as secure co-processors and that GPUs provide a stealthy platform for malware. First, we examine a recent proposal to use discrete GPUs as secure co-processors and show that the security guarantees of the proposed system do not hold on the GPUs we investigate. Second, we demonstrate that (under certain circumstances) it is possible to bypass IOMMU protections and create stealthy, long-lived GPU-based malware. We demonstrate a novel attack that compromises the in-kernel GPU driver and one that compromises GPU microcode to gain full access to CPU physical memory. In general, we find that the highly sophisticated, but poorly documented GPU hardware architecture, hidden behind obscure close-source device drivers and vendor-specific APIs, not only make GPUs a poor choice for applications requiring strong security, but also make GPUs into a security threat.},
booktitle = {Proceedings of the General Purpose GPUs},
pages = {1–11},
numpages = {11},
location = {Austin, TX, USA},
series = {GPGPU-10}
}

@inproceedings {199358,
author = {Tyler Hunt and Zhiting Zhu and Yuanzhong Xu and Simon Peter and Emmett Witchel},
title = {Ryoan: A Distributed Sandbox for Untrusted Computation on Secret Data},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {533--549},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/hunt},
publisher = {USENIX Association},
month = nov,
pdf = {https://www.usenix.org/system/files/conference/osdi16/osdi16-hunt.pdf},
code = {https://github.com/ut-osa/ryoan},
}
